# BR-Lite
Better Rover - Lite is an analysis tool meant to assist in processing large volumes of 802.11 survey data to identify potential co-traveling signals as well as likely static access points and their best guess location. It is being developed on a Raspberry Pi 4 running Raspberry Pi OS Lite (64-Bit) and meant to be run on similarly powered devices.

Intended use: BR-Lite is intended to take raw .kismet and .log.csv files from Kismet and Airodump-NG respectively and iteratively parse data to identify Co Traveling signals and provide summary data for Static Signals, while retaining references to raw data inputs for more specialized analytical uses. It is meant to do this in a lightweight format, requiring only a series of editable scripts, python, a.html viewer, and the ability to reference two online databases (ArcGIS ESRI World Imagery maptiles and eventually the IEEE OUI database) for improved visualization. 


Methodology: BRL is run by executing a master script (brl.sh) from the FIDIM directory which controls the phasing of several sub-processes to iteratively convert, merge, parse, and process data. Users are prompted through the analysis iterations to generate tabular and .hmtl interactive maps to display resulting analysis. It contains the following:


-brl.sh: This is the master script and is executed using ./brl.sh from the FIDIM directory once users have ensured it is executable. It will ensure that all subrodinate scripts are executable, prompt users to execute or skip each subprocess, and runs maintenance tasks (archiving data to remove it from the workflow; destruction to delete everything but source data)


-installer.sh: Checks for or create the required directory structure for input data, intermediate processed data, and output data. Checks for and installs updates, dependencies (currently pyhton3, python3-numpy, python3-folium, python3-pandas, jq), autoremoves unnecesary updates and updates permissions for all .sh and .py scripts permissions (should probably move this to the master script).


-process_source.sh: Updates permissions to all the Input_Data files to remove any read only statuses. Prompts the user to ensure and confirm that source data files have a <sensor>-YYYY-MM-DD naming convention which is used for user nominated date range restrictions for downstream processes. Processes .kismet unified databas files (by using the kismet_to_* commands included with kismet to convert unified databases into wigle.csv formats (which prioritize MAC, SSID, RSSI, Channel, Timestamp, and GPS associations at the expense of all other data), .pcapng files (which are apprently the preferred file for professional use and are compatible and viewable in programs like wireshark (need to analyze these files to determine what data is stored inside and in what format)), .json files (which are JavaScript Object Notation that seem to store in an array a series of objects called dot11.device which appear to be correlated with a detection of a signal during a survey - within this object are a series and subseries of objects and arrays that include various details such as time detected (in Unix epoch time), location detected, advertised BSSID, advertised SSID, probed or connected MAC addresses, etc.), ek.json files which are apparently a single line flattening of .json files that make this kind of data compatible with ELK stack)) and airodump-ng .log.csv files (by searching each record for the earliest and latest timestamps of each record and appended that date-range to the filename to index it for user nominated time-ranges for analysis). It stores these outputs in the processing directory and subdirectories, grouping .kismet outputs into subdirectories that retain the <sensor>-<timesteamp> format to help index, and airdump.log.csv files in their own directory (as they are already indexed).


-co_traveler_merge.sh: this is the first iterative pass of analytical processes which is simply a grouping of data from across sensors, optionally parsing only data from a user defined range. This merge process also removes a bad first row of data on wigle.csv conversion files that have a first row of info meant for upload into a wigle.net database but were often erroneously referenced as column headers in downstream processes. It also removes Longitude and Latitude error from Airodump files (this is mostly due to the fact that we haven't determined how to employ airodump-ng with gps receivers for our survey teams so these columns only create extra noise. It saves these outputs to a Merges directory with a timestamp appended which is intended to allow users to create a database of multi-sensor data from across timeframes and allow users to search by timeframe instead of by aggregate should they desire. This process ensures to "tag" each data entry with its source file entry, in order to allow users to reference back to raw data sources (i.e. the input data from the survey teams) should they need to do detailed analysis.


-co_traveler_analysis.py: this is the second iterative pass of analytical proccess which aims to identify and classify signals as static or co-traveling within an optionally defined time range and output a geographic and tabular visualization of that data for users in the Outputs section. It does this by referencing the merged databases, normalizes column headers from the two input veins (airodump and kismet), and attempts to group unquie MAC addresses (which it treats as synonymous with BSSID (something that may be false though I'm honestly not sure)), with time-stamps, location data, RSSI value at the time of detection, and retains the source file tags). It then separates these into three groups (Potential Co Travler, Static, or Unknown based on the relative location of detections. Unique MAC with any detections >/= 1000m apart are Potential Co Travelers, MACs with all records within 300m of each other are considered static, all others unknown (these definitions are highly biased towards experience in the Anniston region of Alabama where this script was developed - this is the first area in which I plan to try and deploy edge proccessed AI (this is a whole can of worms I'm just opening but it seems like what AI would be good for)). The script then processes this data into a .html interactive map for Potential Co Travelers that references F/OS ArcGIS ESRI World Imagery map tiles, bins MACs by "max follow distance" which is the furthest pairwise distance between any two detections, drops markers that indicate locations a PCT was detected, groups repeated markers into single markers (clustering pass 1) and then uses folium's marker cluster feature for readability. KNOWN ISSUE: This two pass approach causes issues with the markers wherein markers time range displays a first seen and last seen time relevant to the marker, as opposed to the intended purpose which was the first time the MAC was detected out of the entire span of the mapped data. This means that users who click on a map marker are shown time data that is relevant more to when the signal was detected at that location (something that is already known and relatively useless from a security standpoint) instead of giving users an idea of how long a signal may have been following them. There may be value in showing both, though I'm not yet sure how best to both label that data in a way that users will understand (given the small window of information that can be presented in the marker details) or how to calculate that for the script. FUTURE DEV: Because of the nature of the output (proximity over distance), GPS paired data is the only data used in this and other processes however, there are several issues with this. 1) Survey team GPS equipment is fairly insensitive - teams are using BU353N USB connected GPS receivers and are untrained and unaware what methods if any to use to increase or calibrate the sensitivity or accuracy of these readings. Teams are also required to place these receivers in locations that may not provide the best signal quality in order to not be obtrusive or draw unwanted interactions during surveys. 2) The vast majority of the data of interest is contained in location agnostic data sets - location is merely the first filter to determine if a signal is of interest for personal security and general situational awareness. This means that a lot of data is 'left on the table' if the quality of its associated GPS data is poor. In downstream processes I try to call back to this data (particularly in the targeted_analytics.py script) but it would be more useful if it was done here. 


-static_aggregate.py: I still consider this part of the second iterative pass of the analytical process but I broke it out during development since I kept running into issues when attempting to run all these processes in one script. Workflow wise, I think it may be smarter to create a single tabular database with this classification and regressive analysis for Potential Co Travelers, Statics, and Unknowns, calculations of # of detections, max distance, and perhaps a calculation for confidence level of the classification by unique MAC and then focus on creating the geographic and tabular "visualization" outputs as two separate sub-processes. This would also allow the actual analytical process to be more robust (and potentially address the GPS issue) without all the distracting reformatting needed for the visualization processes Regardless, in the current product, this process involves re-referencing the merged Wigle.csv data (as our current survey teams do not know how to enable airodump to reliably capture GPS sensor data) groups signals that are within 200m of one another (this needs to be standardized with the previous process), then attempts to create a best guess location for the signal source using a weighted average of detection coordinates weighted by the associated RSSI values (this is a mathematical process that could be improved but there are several variables to RSSI beyond the physics that may not be worth attempting to account for precisely (namely inconsistent antennae used by survey teams, inconsistent antennae placement between runs, 0 access to refined weather data during runs, and some other things I'm probably not thinking of). Considering the primary end user for this product is for the security and situational awareness of the survey teams themselves, having precise locations for static signals (the vast majority of which are part of civil infrastructre) seems like a low juice-to-squeeze ratio). The script then creates a tabular output meant to be used with the geographic output this time grouping signals by their encryption or security as opposed to follow distance.


-static_signals_map.py: conjoined with the above process, this script merely handles the .html interactive map for static_aggregate.py. I frankly need to rework this entire second pass so I can frontload the actual analytics and then work visualization processes after the fact but considering I built this whole thing in 6 days, that this is a complete rebuild of the MK1 scripts (the development of which is included in those 6 days), and that I'm fundamentally a survey team guy with bachelors in phiolosophy and no data-analytics training or coding experience, I'm willing to accept that failure.

-flagged_signals_analysis: this is a currently broken process but would represent the third iterative pass of analysis. The intent would be to take the flagged signals from the previous pass and further refine their categorization, assigning confidence levels to each assessment and ideally doing so recursively such that as survey teams continue to operate in the field, consistently identified signals could have refined classification. This is second spot in which I think edge processed AI implementation could add value (for the same functional purpose as the first), especially if it's able to refine assessment criteria over time. I'd need to determine how I could build training sets either in the operating environment or at home so I could estimate how much time/effort would be required of survey teams to get this capability online (specifically for survey team leadership to determine their own juice:squeeze ratios (if the process takes 20-30 days and their presence in an area is 15-30, they may not be interested unless the training of the model occurs over several teams and is passed over between survey groups, but this beyond my level).


-targeted_analytics: this process is meant to allow users to take a signal (classified based on MAC or SSID) identified in the second and third iterative passes and recursively searches for every available data point associated with that identifier across all aggregated data (including the JSON files). Consider how fluid both MAC and SSIDs can be, this needs a great deal of work and represents the second functional area where AI implementation could be useful though I'm completely unsure how I would implement this. 
